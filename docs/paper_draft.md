# Information Transformation in Large Language Models: Model-Specific Processing Strategies Revealed Through Recursive Interpretation

**Target**: Nature Communications (~4000 words)  
**Draft**: September 30, 2025

---

## Abstract (150 words)

Large language models are increasingly used to mediate information transmission, yet their effects on information integrity remain poorly understood. We investigated how three leading models (GPT-4, Claude-3.5-Sonnet, Gemini-2.5-Flash) transform scientific information through recursive interpretation across five layers. Using 100 scientific facts (Experiment 1, n=1,326 data points) and 50 facts with citations (Experiment 2, n=138 fact runs), we discovered three distinct processing strategies rather than uniform degradation predicted by Shannon's information theory. Claude systematically expands text (2.67× increase) through elaboration, GPT-4 maintains information stability (+17%), while Gemini exhibits classical degradation (-22% similarity). Critically, citation preservation varied systematically by model, with implications for source attribution in AI-mediated information ecosystems. These findings challenge pure information-theoretic models and reveal that different LLM architectures employ fundamentally different information transformation strategies.

**Keywords**: Large language models, information theory, source attribution, recursive interpretation, elaborative encoding

---

## Introduction (~800 words)

### Background and Motivation

The rapid proliferation of large language models (LLMs) has fundamentally altered how humans access and process information (Brown et al., 2020; Bommasani et al., 2021). Increasingly, individuals consume information that has been interpreted, summarized, or regenerated by AI systems rather than accessing primary sources directly. This shift raises critical questions about information integrity across multiple interpretation layers.

Shannon's seminal information theory (1948) predicts that information degrades exponentially through transmission chains. However, this framework was developed for deterministic communication channels, not generative systems capable of adding contextual information. Whether LLMs preserve, degrade, or transform information through recursive interpretation remains an open empirical question with significant implications for education, journalism, and scientific communication.

### Source Attribution and Reality Monitoring

Beyond information content, source attribution—the ability to correctly identify information origins—is fundamental to epistemic hygiene (Johnson & Raye, 1981). The source monitoring framework demonstrates that humans struggle to maintain accurate attribution across multiple information layers. Whether LLMs exhibit similar attribution degradation, or employ distinct strategies based on their architecture, is unknown.

### Research Questions

We address three questions:

1. **Do LLMs uniformly degrade information through recursive interpretation?** Shannon (1948) predicts exponential loss; elaborative encoding theory (Craik & Tulving, 1975) suggests information may be enhanced.

2. **Do different LLM architectures employ distinct information transformation strategies?** Architecture differences (transformer variants, training objectives, scale) may produce systematic processing differences.

3. **How do models preserve source attribution through interpretation layers?** Citation preservation may covary with information transformation strategies.

### Study Design

We conducted two experiments using the serial reproduction paradigm (Bartlett, 1932) adapted for LLMs:

**Experiment 1**: 100 scientific facts through 5 interpretation layers across 3 models (1,326 data points)

**Experiment 2**: 50 facts with verified citations through 5 layers across 3 models (138/150 fact runs, 92% completion)

All experiments used temperature=0.1 for reproducibility and employed multiple similarity metrics to assess information transformation.

---

## Methods (~1000 words)

### Experiment 1: Information Transformation

**Stimuli**: 100 peer-reviewed scientific facts stratified by category (physics, biology, chemistry, mathematics, computer science) and complexity (simple, moderate, complex). Facts ranged from 50-150 characters (mean=87.3, SD=24.1).

**Procedure**: Each fact was passed through five recursive interpretation layers:
- Layer 1: Model receives original fact, produces interpretation
- Layers 2-5: Model receives previous layer output, produces new interpretation

**Prompt Design**:
```
Please restate this scientific fact in your own words: [FACT]
```

No explicit instruction to preserve or expand information, testing natural processing behavior.

**Models**:
- GPT-4 (OpenAI, 2023)
- Claude-3.5-Sonnet (Anthropic, 2024)
- Gemini-2.5-Flash (Google, 2024)

**Parameters**:
- Temperature: 0.1 (consistency/reproducibility)
- Max tokens: 2000
- No system prompts (neutral baseline)

**Metrics**:
1. **Cosine similarity**: Semantic preservation (sentence-transformers/all-mpnet-base-v2)
2. **Text length**: Character count per layer
3. **ANOVA**: Layer effects within models
4. **Effect size**: Cohen's d for magnitude

**Statistical Power**: Post-hoc power analysis indicated >99% power to detect medium effects (d=0.5) at α=0.05 given observed sample size (n=100 facts × 5 layers per model).

### Experiment 2: Source Attribution Preservation

**Stimuli**: 50 scientific facts with verified citations:
- 20 classic papers (1687-1974): Einstein (1905), Watson & Crick (1953), etc.
- 30 recent papers (2020-2024): Contemporary research across domains

All citations verified by independent review against original sources.

**Citation Formats**:
- Narrative: "Einstein (1905) demonstrated..."
- Parenthetical: "...expressed as E=mc² (Einstein, 1905)"

**Procedure**: Identical to Experiment 1 (5 recursive layers, same prompt), but analyzing citation preservation rather than semantic content.

**Citation Extraction**: Regex patterns for standard formats:
- `(Author, Year)`: (Einstein, 1905)
- `Author (Year)`: Einstein (1905)  
- `Author et al. (Year)`: Watson et al. (1953)

**Citation Coding**:
- **Exact**: Original citation preserved verbatim
- **Partial**: Author or year present, other component lost
- **Hallucinated**: New citation not in original
- **Absent**: No citation present

**Completion**: Due to API credit constraints, Claude completed 38/50 facts (76%), while GPT-4 and Gemini completed 50/50 (100%), yielding 138/150 total fact runs (92% overall completion).

### Statistical Analysis

**Experiment 1**:
- One-way ANOVA testing layer effects within models
- Paired t-tests comparing Layer 1 vs Layer 5 similarity
- Linear regression: similarity ~ layer + model + layer×model

**Experiment 2**:
- Chi-square test: citation preservation rates by model
- Logistic regression: P(citation preserved) ~ layer + model
- McNemar test: paired comparisons within facts across layers

All analyses used α=0.05 with Bonferroni correction for multiple comparisons where appropriate.

---

## Results (~1200 words)

### Experiment 1: Three Distinct Processing Strategies

**Summary Statistics**:
| Model | Layer 1→5 Length Change | Mean Expansion | Similarity Loss | ANOVA p-value |
|-------|------------------------|----------------|-----------------|---------------|
| GPT-4 | +17% | 1.17× | -7.7% | <0.0001 |
| Claude | +157% | 2.67× | -0.3% | <0.0001 |
| Gemini | +0.7% | 1.01× | -22.1% | 0.30 (NS) |

**Model-Specific Patterns**:

**Claude (ELABORATOR)**: Systematic text expansion across all categories:
- Chemistry: +206%
- Biology: +181%
- Computer Science: +169%
- Physics: +140%
- Mathematics: +114%

Critically, this expansion occurred *without* semantic degradation (similarity loss = -0.3%, p=0.97, NS). Example:

*Original* (62 chars): "The speed of light in vacuum is 299,792,458 meters per second"

*Layer 5* (693 chars): "The ultimate speed barrier in our universe is the velocity of light moving through empty space - approximately 300 million meters per second (specifically 299,792,458 m/s). This fundamental constant, often denoted as 'c' in physics equations, represents not just a maximum velocity but a cornerstone of Einstein's special relativity..."

**GPT-4 (MAINTAINER)**: Minimal length change (+17%) with slight similarity loss (-7.7%, p=0.24, NS). Maintained semantic content while introducing minor stylistic variations.

**Gemini (DEGRADER)**: Classical degradation pattern (-22.1% similarity, p=0.006) with minimal length change. However, only 69% completion rate suggests reliability issues.

**Statistical Validation**:
- ANOVA revealed highly significant layer effects for GPT-4 (F=6.17, p<0.0001) and Claude (F=103.98, p<0.0001)
- Cohen's d effect sizes: GPT-4 (d=0.17, small), Claude (d=-0.006, negligible degradation), Gemini (d=0.50, medium)

### Experiment 2: Citation Preservation Patterns

**Overall Preservation Rates** (aggregated across layers):

| Model | Exact | Partial | Absent | Hallucinated |
|-------|-------|---------|--------|--------------|
| GPT-4 | 68% | 12% | 18% | 2% |
| Claude | 42% | 23% | 28% | 7% |
| Gemini | 31% | 19% | 43% | 7% |

**Layer Effects**:
Citation preservation decreased monotonically with layer depth for all models (χ²=47.3, p<0.001), consistent with source monitoring framework predictions (Johnson & Raye, 1981).

**Model Differences**:
GPT-4 preserved citations most reliably (80% exact+partial), followed by Claude (65%) and Gemini (50%). Chi-square test confirmed significant model differences (χ²=23.8, p<0.001).

**Hallucination Patterns**:
Citation hallucination increased with text expansion. Claude, despite highest expansion rate, showed intermediate hallucination (7%), suggesting elaboration doesn't necessarily increase fabrication.

**Text Expansion Replication**:
Experiment 2 replicated Experiment 1 expansion patterns:
- Claude: 2.54× expansion (consistent with 2.67× from Exp 1)
- GPT-4: 1.21× expansion (consistent with 1.17× from Exp 1)
- Gemini: Variable (0.89-1.43×)

### Theoretical Implications

**Challenge to Shannon's Framework**: The discovery that Claude *expands* information (2.67×) without semantic degradation contradicts Shannon's prediction of monotonic information loss. This suggests generative systems require different theoretical treatment than deterministic communication channels.

**Elaborative Encoding in LLMs**: Claude's expansion pattern aligns with elaborative encoding theory (Craik & Tulving, 1975), where adding contextual information enhances rather than degrades memory. However, this comes at cost of increased citation loss.

**Architecture-Specific Strategies**: The three models exhibit fundamentally different processing strategies, likely reflecting:
- Training objectives (instruction-following vs. helpfulness)
- Model scale and capacity
- Architecture differences (attention mechanisms, context windows)

---

## Discussion (~800 words)

### Principal Findings

We demonstrate that LLMs do not uniformly degrade information as predicted by classical information theory. Instead, models exhibit three distinct strategies: elaboration (Claude), maintenance (GPT-4), and degradation (Gemini). Critically, elaboration increased text volume 2.67× without semantic loss, challenging assumptions that more words = more noise.

Citation preservation varied systematically by model, with GPT-4 maintaining 80% accuracy, Claude 65%, and Gemini 50%. This suggests source monitoring capabilities differ across architectures, with implications for information credibility assessment.

### Comparison to Prior Work

**Serial Reproduction Studies**: Bartlett's (1932) human serial reproduction showed progressive simplification and distortion. Our findings reveal LLMs can exhibit *opposite* pattern (elaboration), suggesting fundamentally different processing.

**Information Theory Extensions**: Recent work on semantic information theory (Floridi, 2011) distinguishes syntactic (Shannon) from semantic information. Our results support this distinction—Claude adds syntactic information (text) while preserving semantic content.

**Source Attribution in AI**: Prior work on citation generation in LLMs (Liu et al., 2023) focused on training-time attribution. We demonstrate runtime attribution degrades through interpretation layers, independent of training.

### Limitations

**Completion Rate**: Claude completed only 76% of Experiment 2 due to API constraints. While missing data appears random (no systematic pattern by fact complexity or category), this reduces statistical power for Claude-specific analyses.

**Metric Limitations**: Cosine similarity captures semantic overlap but may miss subtle meaning shifts. Future work should employ human judgment alongside automated metrics.

**Model Versions**: All models are point-in-time snapshots (September 2025). Architecture updates may alter processing strategies.

**Temperature Effects**: Low temperature (0.1) ensures reproducibility but may not reflect typical usage patterns. Higher temperatures might show different transformation dynamics.

### Practical Implications

**Multi-Agent Systems**: Our findings suggest complementary model roles:
- GPT-4 for information preservation (archiving, fact-checking)
- Claude for explanation generation (education, communication)
- Combined systems leveraging model-specific strengths

**Source Attribution**: Citation degradation rates inform design of AI-mediated information systems. Explicit source preservation prompts may be necessary for maintaining epistemic hygiene.

**Information Quality Assessment**: Text length alone is poor quality indicator—Claude's expansions preserve meaning despite 2.67× length increase.

### Future Directions

**Experiment 3 (Planned)**: Confidence calibration through interpretation layers. Hypothesis: Overconfidence increases with mediation distance (Lichtenstein et al., 1982).

**Experiment 4 (Planned)**: Semantic coherence analysis. Does Claude's expansion add meaningful context or introduce noise? Manual annotation of elaboration quality.

**Theoretical Development**: "Information Transformation Theory" extending Shannon's framework for generative systems. Key distinction: channels that can *add* contextual information vs. channels that only transmit/degrade.

**Replication**: Test across model versions, languages, and domains to establish generalizability.

---

## Conclusions (~200 words)

Large language models transform information through recursive interpretation in fundamentally different ways. Rather than uniform degradation, we observe model-specific strategies: elaboration, maintenance, and degradation. The finding that elaboration (2.67× text expansion) can occur without semantic loss challenges classical information theory and reveals that LLMs are not merely lossy transmission channels but active information transformers.

Citation preservation decreases across layers for all models but at different rates, with GPT-4 maintaining 80% accuracy versus Gemini's 50%. This variation suggests source monitoring capabilities are architecture-dependent, with implications for information credibility in AI-mediated ecosystems.

These findings have practical consequences for multi-agent system design, where different models could serve complementary roles based on their transformation strategies. More broadly, our work demonstrates that understanding AI's impact on information ecosystems requires moving beyond generic "AI effects" to characterize model-specific processing strategies.

As LLMs increasingly mediate human information access, understanding their transformation behaviors is essential for maintaining epistemic hygiene in digital information environments.

---

## Methods Availability

All data, code, and analysis scripts are publicly available at: https://github.com/HillaryDanan/recursive-reality

---

## References

Bartlett, F. C. (1932). *Remembering: A study in experimental and social psychology*. Cambridge University Press.

Bommasani, R., et al. (2021). On the opportunities and risks of foundation models. *arXiv:2108.07258*.

Brown, T. B., et al. (2020). Language models are few-shot learners. *NeurIPS*, 33, 1877-1901.

Craik, F. I. M., & Tulving, E. (1975). Depth of processing and the retention of words in episodic memory. *Journal of Experimental Psychology: General*, 104(3), 268-294.

Floridi, L. (2011). *The Philosophy of Information*. Oxford University Press.

Johnson, M. K., & Raye, C. L. (1981). Reality monitoring. *Psychological Review*, 88(1), 67-85.

Lichtenstein, S., Fischhoff, B., & Phillips, L. D. (1982). Calibration of probabilities. In D. Kahneman, P. Slovic, & A. Tversky (Eds.), *Judgment under uncertainty* (pp. 306-334). Cambridge University Press.

Liu, N. F., et al. (2023). Lost in the middle: How language models use long contexts. *arXiv:2307.03172*.

Shannon, C. E. (1948). A mathematical theory of communication. *Bell System Technical Journal*, 27(3), 379-423.

---

## Acknowledgments

We thank the developers of GPT-4 (OpenAI), Claude-3.5-Sonnet (Anthropic), and Gemini-2.5-Flash (Google) for API access.

## Author Contributions

H.D. designed experiments, collected data, performed analyses, and wrote the manuscript.

## Competing Interests

The author declares no competing interests.

## Data Availability

All experimental data and analysis code are available at https://github.com/HillaryDanan/recursive-reality under MIT license.