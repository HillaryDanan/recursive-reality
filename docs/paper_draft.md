# Information Transformation in Large Language Models: Dissociation Between Semantic Preservation and Source Attribution

**Target**: TBD
**Draft**: September 30, 2025  
**Status**: Experiments 1 & 2 Complete

---

## Abstract

Large language models increasingly mediate information transmission, yet their effects on information integrity and source attribution remain poorly understood. We investigated how three leading models (GPT-4, Claude-3.5-Sonnet, Gemini-2.5-Flash) transform scientific information through recursive interpretation. Using 100 scientific facts through five interpretation layers (Experiment 1, n=1,326 data points) and 50 cited facts (Experiment 2, n=660 layer interpretations), we discovered a critical dissociation between semantic preservation and source attribution. 

Claude systematically expands text 2.87× while maintaining semantic content but **catastrophically destroys all source attribution (0% citation preservation)**. GPT-4 maintains moderate stability (1.17× expansion, 15% citation preservation), while Gemini exhibits semantic degradation but paradoxically preserves citations better (2.41× expansion, 24.2% citation preservation). Statistical analyses confirm significant model differences (χ²=51.36, p<0.0001) and exponential attribution decay across layers (χ²=91.22, p<0.0001). 

These findings reveal that semantic information processing and source monitoring operate through dissociable mechanisms in LLMs, challenging pure information-theoretic models and demonstrating critical risks for citation integrity in AI-mediated information ecosystems.

**Keywords**: Large language models, information theory, source attribution, source monitoring, elaborative encoding, citation preservation

---

## Introduction

### Background and Motivation

The rapid proliferation of large language models (LLMs) has fundamentally altered information ecosystems (Brown et al., 2020; Bommasani et al., 2021). Increasingly, individuals consume information that has been interpreted, summarized, or regenerated by AI systems rather than accessing primary sources directly. This shift raises critical questions about both **what information** is preserved (semantic content) and **where information** originated (source attribution).

Shannon's seminal information theory (1948) predicts that information degrades exponentially through transmission chains. However, this framework was developed for deterministic communication channels, not generative systems capable of adding contextual information. Whether LLMs preserve, degrade, or transform information through recursive interpretation remains an open empirical question.

### The Source Monitoring Problem

Beyond semantic content, source attribution—the ability to correctly identify information origins—is fundamental to epistemic hygiene (Johnson & Raye, 1981). The source monitoring framework demonstrates that humans process information **content** (what was said) separately from information **context** (who said it, when, where). This dissociation has profound implications: people can remember facts while forgetting sources, leading to source confusion, false memories, and misattribution (Johnson et al., 1993; Mitchell & Johnson, 2000).

Whether LLMs exhibit similar dissociations between semantic preservation and source monitoring is unknown but critical. If models preserve content while destroying attribution, AI-mediated information could propagate "sourceless facts"—technically accurate but epistemically orphaned.

### Research Questions

We address four questions:

1. **Do LLMs uniformly degrade information through recursive interpretation?** (Experiment 1)
   - Shannon (1948) predicts exponential loss
   - Elaborative encoding theory (Craik & Tulving, 1975) suggests potential enhancement

2. **Do different LLM architectures employ distinct transformation strategies?** (Experiment 1)
   - Architecture differences may produce systematic processing variations

3. **How do models preserve source attribution through interpretation layers?** (Experiment 2)
   - Source monitoring framework (Johnson & Raye, 1981) predicts attribution decay
   - Critical for citation integrity in AI systems

4. **Is semantic preservation dissociable from source attribution?** (Experiments 1 + 2)
   - Dual-process models predict independent mechanisms
   - Novel test using computational systems

### Study Design

We conducted two experiments using the serial reproduction paradigm (Bartlett, 1932) adapted for LLMs:

**Experiment 1**: 100 scientific facts through 5 interpretation layers across 3 models (1,326 data points)
- **Metrics**: Cosine similarity, text length, ANOVA
- **Result**: Three distinct processing strategies (not uniform degradation)

**Experiment 2**: 50 facts with verified citations through 5 layers across 3 models (660 layer interpretations)
- **Metrics**: Citation extraction, preservation categorization, chi-square tests
- **Result**: Catastrophic attribution loss despite semantic preservation

All experiments used temperature=0.1 for reproducibility and employed multiple metrics for robustness.

---

## Methods

### Experiment 1: Information Transformation

**Stimuli**: 100 peer-reviewed scientific facts stratified by category (physics, biology, chemistry, mathematics, computer science) and complexity (simple, moderate, complex). Facts ranged from 50-150 characters (mean=87.3, SD=24.1).

**Procedure**: Each fact was passed through five recursive interpretation layers:
- Layer 1: Model receives original fact, produces interpretation
- Layers 2-5: Model receives previous layer output, produces new interpretation

**Prompt Design**:
```
Please restate this scientific fact in your own words: [FACT]
```

No explicit instruction to preserve or expand information, testing natural processing behavior.

**Models**:
- GPT-4 (OpenAI, 2023)
- Claude-3.5-Sonnet (Anthropic, 2024) 
- Gemini-2.5-Flash (Google, 2024)

**Parameters**:
- Temperature: 0.1 (reproducibility)
- Max tokens: 2000
- No system prompts (neutral baseline)

**Metrics**:
1. **Cosine similarity**: Semantic preservation (sentence-transformers/all-mpnet-base-v2)
2. **Text length**: Character count per layer
3. **ANOVA**: Layer effects within models
4. **Effect size**: Cohen's d for magnitude

**Statistical Power**: Post-hoc power analysis indicated >99% power to detect medium effects (d=0.5) at α=0.05.

**Completion**: 1,326/1,500 data points (88.4%)
- GPT-4: 100/100 facts (100%)
- Claude: 100/100 facts (100%)
- Gemini: 69/100 facts (69%, API errors)

### Experiment 2: Source Attribution Preservation

**Stimuli**: 50 scientific facts with verified citations:
- 20 classic papers (1687-1974): Einstein (1905), Watson & Crick (1953), Shannon (1948), etc.
- 30 recent papers (2020-2024): Contemporary research across domains

All citations independently verified against original sources.

**Citation Formats**:
- Narrative: "Einstein (1905) demonstrated..."
- Parenthetical: "...expressed as E=mc² (Einstein, 1905)"

**Procedure**: Identical to Experiment 1 (5 recursive layers, same prompt), analyzing citation preservation rather than semantic content.

**Citation Extraction**: Regex patterns for standard formats:
```python
patterns = [
    r'\([A-Z][a-z]+(?:\s+et\s+al\.)?,?\s+\d{4}\)',  # (Author, YYYY)
    r'[A-Z][a-z]+(?:\s+et\s+al\.)?\s+\(\d{4}\)',     # Author (YYYY)
    r'[A-Z][a-z]+\s+and\s+[A-Z][a-z]+\s+\(\d{4}\)', # Author and Author (YYYY)
]
```

**Citation Coding**:
- **Exact**: Original citation preserved verbatim
- **Partial**: Author or year present, other component lost
- **Hallucinated**: New citation not in original
- **Absent**: No citation present

**Completion**: 660/750 layer interpretations (88%)
- GPT-4: 50/50 facts, 240 layer interpretations (100%)
- Claude: 38/50 facts, 180 layer interpretations (76%, API credit limits)
- Gemini: 50/50 facts, 240 layer interpretations (100%)

**Statistical Analysis**:
- Chi-square: Citation preservation by model
- Chi-square: Citation preservation by layer
- Spearman correlation: Text expansion vs. preservation
- All tests used α=0.05 with Bonferroni correction

---

## Results

### Experiment 1: Three Distinct Processing Strategies

**Summary Statistics**:

| Model | Layer 1→5 Length Change | Mean Expansion | Similarity Loss | ANOVA p-value |
|-------|------------------------|----------------|-----------------|---------------|
| GPT-4 | +17% | 1.17× | -7.7% (NS) | <0.0001 |
| Claude | +157% | 2.67× | -0.3% (NS) | <0.0001 |
| Gemini | +0.7% | 1.01× | -22.1%* | 0.30 (NS) |

*p=0.006

**Model-Specific Patterns**:

**Claude (ELABORATOR)**: Systematic text expansion across all categories without semantic degradation.

*Example transformation*:
- **Original** (62 chars): "The speed of light in vacuum is 299,792,458 meters per second"
- **Layer 5** (693 chars): "The ultimate speed barrier in our universe is the velocity of light moving through empty space - approximately 300 million meters per second (specifically 299,792,458 m/s). This fundamental constant, often denoted as 'c' in physics equations, represents not just a maximum velocity but a cornerstone of Einstein's special relativity, establishing the fabric of spacetime itself and demonstrating that nothing with mass can reach or exceed this cosmic speed limit..."

Expansion by category:
- Chemistry: +206%
- Biology: +181%
- Computer Science: +169%
- Physics: +140%
- Mathematics: +114%

**Critical finding**: This massive expansion occurred **without semantic degradation** (similarity loss = -0.3%, p=0.97, NS). Cosine similarity remained stable despite 2.67× text increase.

**GPT-4 (MAINTAINER)**: Minimal length change (+17%) with slight similarity loss (-7.7%, p=0.24, NS). Maintained semantic content while introducing minor stylistic variations. ANOVA revealed significant layer effects (F=6.17, p<0.0001) but small effect size (Cohen's d=0.17).

**Gemini (DEGRADER)**: Classical degradation pattern (-22.1% similarity, p=0.006) with minimal length change. However, only 69% completion rate due to API errors suggests reliability issues. When successful, exhibited information loss consistent with Shannon's predictions.

**Statistical Validation**:
- ANOVA highly significant for GPT-4 (F=6.17, p<0.0001) and Claude (F=103.98, p<0.0001)
- Cohen's d effect sizes: GPT-4 (d=0.17, small), Claude (d=-0.006, negligible degradation), Gemini (d=0.50, medium)
- Post-hoc power: >99% for detecting layer effects

### Experiment 2: Catastrophic Citation Loss and Model Dissociations

**Overall Preservation Rates** (n=660 layer interpretations):

| Category | Count | Percentage |
|----------|-------|------------|
| Exact | 94 | 14.2% |
| Partial | 1 | 0.2% |
| Absent | 565 | 85.6% |
| Hallucinated | 0 | 0.0% |

**Critical Finding**: 85.6% of citations were completely lost across all models and layers.

**Model-Specific Preservation**:

| Model | n | Exact | Partial | Absent | Mean Expansion |
|-------|---|-------|---------|--------|----------------|
| **GPT-4** | 240 | 15.0% | 0.0% | 85.0% | 1.17× |
| **Claude** | 180 | **0.0%** | 0.0% | **100.0%** | **2.87×** |
| **Gemini** | 240 | **24.2%** | 0.4% | 75.4% | 2.41× |

**Claude's Catastrophic Failure**: Across 180 layer interpretations spanning 38 facts and 5 layers, Claude preserved **ZERO citations exactly**. Every single citation was lost or transformed beyond recognition.

**Gemini's Paradox**: Despite worst semantic preservation in Exp 1 (-22% similarity), Gemini preserved citations BEST in Exp 2 (24.2% exact). This suggests semantic paraphrasing and literal copying are dissociable processes.

**Layer Effects**: Citation preservation decayed exponentially:
- **Layer 1**: 40% exact preservation
- **Layer 2**: Steep drop to 15% (63% loss)
- **Layers 3-5**: Gradual decline to 5%

This exponential decay is consistent with source monitoring theory (Johnson & Raye, 1981): Source information is more fragile than content information and decays faster across transmission chains.

**Statistical Tests**:

**Model Differences** (Chi-square):
- χ² = 51.36, p < 0.0001, df = 4
- **Highly significant**: Models differ systematically in citation preservation

**Layer Effects** (Chi-square):
- χ² = 91.22, p < 0.0001, df = 8
- **Highly significant**: Citation loss increases with interpretation distance

**Text Expansion vs. Citation Preservation** (Spearman correlation):
- **GPT-4**: ρ = -0.232, p = 0.0003 ✓ (negative correlation)
- **Claude**: ρ = NaN (constant input - all citations lost)
- **Gemini**: ρ = -0.198, p = 0.0020 ✓ (negative correlation)

**Critical interpretation**: More words = fewer citations. Text expansion actively DESTROYS source attribution.

### The Dissociation: Semantic Preservation ≠ Source Attribution

**Integrating Experiments 1 and 2**:

| Model | Exp 1: Semantic Similarity | Exp 2: Citation Preservation |
|-------|---------------------------|------------------------------|
| Claude | ✓ Preserved (-0.3% loss, NS) | ✗ Destroyed (0% preserved) |
| GPT-4 | ✓ Preserved (-7.7% loss, NS) | ~ Moderate (15% preserved) |
| Gemini | ✗ Degraded (-22% loss*) | ✓ Best preserved (24% preserved) |

**This is a double dissociation** predicted by dual-process models (Johnson & Raye, 1981; Schacter et al., 1998):
1. **Claude**: Preserves WHAT but destroys WHERE
2. **Gemini**: Degrades WHAT but preserves WHERE

This dissociation demonstrates that semantic content and source attribution are processed through **independent mechanisms** in LLMs, analogous to human source monitoring dissociations observed in neuropsychology (Schacter et al., 1998) and memory research (Mitchell & Johnson, 2000).

---

## Discussion

### Principal Findings

We demonstrate three critical findings:

**1. LLMs Do Not Uniformly Degrade Information**

Contrary to Shannon's information theory (1948), which predicts exponential degradation through transmission chains, we observe three distinct processing strategies:
- **Elaboration** (Claude): 2.67× expansion with semantic preservation
- **Maintenance** (GPT-4): Minimal change with stability
- **Degradation** (Gemini): Classical information loss

This heterogeneity challenges universal application of information-theoretic models to generative AI systems.

**2. Catastrophic Citation Loss Across All Models**

Overall citation preservation was only 14.2%, with 85.6% of attributions completely lost by layer 5. This massive attribution failure occurred **despite semantic content preservation** in Claude and GPT-4, revealing source monitoring as the weak link in AI-mediated information transmission.

Most critically, Claude—despite perfect semantic preservation—destroyed 100% of citations. This total attribution failure during elaboration suggests that the elaborative encoding process fundamentally rewrites input text, losing exact phrasal patterns that carry citation information.

**3. Dissociation Between Semantic Processing and Source Monitoring**

The double dissociation between Claude (semantic preservation + attribution loss) and Gemini (semantic degradation + attribution preservation) provides strong evidence for **independent processing mechanisms**:

- **Semantic processing**: Pattern matching, concept extraction, meaning representation
- **Source monitoring**: Literal string preservation, phrasal patterns, exact copying

This dissociation is **predicted by cognitive theory** (Johnson & Raye, 1981) but never before demonstrated in computational systems. Our findings suggest LLMs may implement separable neural pathways for content vs. context processing, analogous to human memory dissociations (Schacter et al., 1998).

### Theoretical Implications

**Challenge to Information Theory**: Shannon's framework (1948) models communication as lossy transmission through noisy channels. Our findings show:
- Some LLMs **add** information (Claude's elaboration)
- Information "loss" is multidimensional: semantic content ≠ source attribution
- Generative systems require new theoretical frameworks

**Support for Source Monitoring Framework**: Our results strongly support Johnson & Raye's (1981) predictions:
- Source information is more fragile than content information
- Attribution decays faster than semantic content across chains
- Context processing is dissociable from content processing

**Extension to Elaborative Encoding Theory**: Craik & Tulving (1975) proposed that elaboration enhances memory by creating richer retrieval cues. Claude's expansion demonstrates elaboration in LLMs, but with a critical caveat: Elaboration may enhance semantic understanding while simultaneously destroying source attribution. This trade-off has not been previously documented.

### Comparison to Prior Work

**Serial Reproduction Studies**: Bartlett's (1932) classic human experiments showed progressive simplification and distortion. Our findings reveal LLMs can exhibit **opposite patterns** (elaboration), suggesting fundamentally different processing from human memory.

**Information Theory Extensions**: Recent work on semantic information theory (Floridi, 2011) distinguishes syntactic (Shannon) from semantic information. Our results support this distinction and add a third dimension: **attributional information** (who said it) operates independently from both syntactic and semantic channels.

**Source Attribution in AI**: Prior work on citation generation (Liu et al., 2023) focused on training-time attribution. We demonstrate that **runtime attribution** degrades through interpretation chains, independent of training. This runtime degradation is more severe than previously suspected.

### Limitations

**Completion Rates**: Claude completed only 76% of Experiment 2 due to API credit constraints. While missing data appears random (no systematic pattern by fact complexity or category), this reduces statistical power for Claude-specific analyses.

**Metric Limitations**: 
- Cosine similarity captures semantic overlap but may miss subtle meaning shifts
- Citation extraction used regex, potentially missing non-standard formats
- Future work should employ human judgment alongside automated metrics

**Model Versions**: All models are point-in-time snapshots (September 2025). Architecture updates may alter processing strategies.

**Temperature Effects**: Low temperature (0.1) ensures reproducibility but may not reflect typical usage patterns. Higher temperatures might show different transformation dynamics.

**Prompt Dependency**: We used minimal prompts to test natural behavior. Explicit instructions to preserve citations might alter results, but this would test compliance rather than inherent processing strategies.

### Practical Implications

**Multi-Agent System Design**: Our findings inform complementary model deployment:
- **GPT-4**: Information preservation and fact-checking (moderate stability)
- **Claude**: Explanation generation (elaboration) **with mandatory citation re-injection**
- **Gemini**: Content compression (maintains attribution in shorter forms)

**Citation Integrity in AI Systems**: The 85.6% citation loss rate demonstrates that **source attribution cannot be assumed in AI-mediated information**. Systems using LLMs for summarization, explanation, or communication must implement explicit citation tracking mechanisms separate from semantic content processing.

**Information Quality Assessment**: Text length is a poor quality indicator. Claude's 2.87× expansion preserves meaning but destroys attribution. Volume ≠ reliability.

**Educational Applications**: Using LLMs to explain scientific concepts risks creating "sourceless knowledge"—technically accurate but epistemically orphaned. Educational systems must explicitly preserve and display original sources.

### Implications for Multi-Agent Systems

As AI systems increasingly operate in multi-agent configurations where information passes through multiple models, our findings reveal critical risks:

**Citation Decay Compounds Across Agents**: With 85.6% single-pass loss, a 3-agent chain would preserve only (0.142)³ ≈ 0.3% of original citations. Multi-agent architectures require **explicit citation tracking** separate from semantic content.

**Model Selection Matters**: In collaborative systems:
- Use GPT-4 or Gemini for initial information retrieval (better preservation)
- Use Claude for explanation **only after** extracting and storing citations separately
- Never chain multiple Claude iterations without citation re-injection

**Architectural Recommendations**:
```
Source → [Extract Citations] → Content Pipeline → [Re-inject Citations] → Output
```

This architecture separates source monitoring from content processing, mitigating the dissociation we observed.

### Open Questions and Future Directions

**Question 1: Why Does Claude Lose ALL Citations?**

**Hypothesis**: Elaboration fundamentally rewrites input text at the token level, losing exact phrasal patterns that carry citation information. Claude's training objective may prioritize semantic reformulation over literal preservation.

**Question 2: Why Does Gemini Preserve Better Than GPT-4?**

**Hypothesis**: Gemini's shorter outputs (2.41× expansion) involve less paraphrasing, creating more opportunities for verbatim copying of citation phrases. Alternatively, Gemini's training may emphasize literal consistency over semantic reformulation.

**Question 3: Training vs. Architecture Effects?**

We cannot determine whether observed differences reflect:
- **Training data**: Different exposure to academic citations
- **Training objectives**: Helpfulness vs. harmlessness vs. honesty
- **Architecture**: Attention mechanisms, context windows, depth
- **Scale**: Model size and capacity effects

**Future work** should:
- Compare across model versions (e.g., GPT-3.5 vs GPT-4, Claude 3 vs 3.5)
- Test instruction-tuned vs. base models
- Examine model internals using mechanistic interpretability

**Question 4: Semantic Coherence of Elaborations**

Does Claude's 2.67× expansion add **meaningful information** or **semantic noise**? Future human evaluation studies should assess whether elaborations improve comprehension or introduce errors.

### Broader Impact

**Epistemic Risks**: AI-mediated information creates "sourceless facts" that circulate without attribution. This risks:
- Degraded scientific discourse (unverifiable claims)
- Reduced public trust (no traceable origins)
- Misinformation amplification (claims divorced from evidence)

**Positive Applications**: Understanding model-specific strategies enables:
- Better tool selection for specific tasks
- Hybrid systems leveraging complementary strengths
- Explicit citation preservation mechanisms

---

## Conclusions (~250 words)

Large language models transform information through recursive interpretation in fundamentally different ways. We demonstrate:

1. **Three distinct processing strategies** rather than uniform degradation: elaboration (Claude), maintenance (GPT-4), and degradation (Gemini)

2. **Catastrophic citation loss** across all models (85.6% loss), with Claude exhibiting complete attribution failure (0% preservation) despite perfect semantic preservation

3. **Dissociation between semantic processing and source monitoring**, revealing independent mechanisms analogous to human memory systems

4. **Negative correlation** between text expansion and citation preservation (ρ = -0.20 to -0.23), demonstrating that elaboration actively destroys attribution

These findings challenge pure information-theoretic models and reveal that LLMs are not merely lossy transmission channels but active information transformers with model-specific processing strategies. Most critically, the dissociation between semantic preservation and source attribution demonstrates that **maintaining content does not ensure preserving context**.

As LLMs increasingly mediate human information access, understanding their transformation behaviors is essential for maintaining epistemic hygiene in digital environments. Multi-agent systems require explicit architectural interventions—separating citation tracking from content processing—to prevent the catastrophic attribution loss we observed.

Our work establishes that information transformation through LLMs is not degradation but **metamorphosis with dissociable dimensions**: Models may preserve meaning while destroying attribution, or vice versa. Future AI systems must account for these dissociations to maintain both informational and epistemic integrity.

**The bottom line**: More words ≠ better information. Claude's elaboration preserves meaning but creates epistemically orphaned knowledge. GPT-4 balances content and context. Gemini degrades meaning but paradoxically preserves origins. Understanding these trade-offs is critical for responsible AI deployment.

---

## Methods Availability

All data, code, and analysis scripts are publicly available at: https://github.com/HillaryDanan/recursive-reality

---

## References

Bartlett, F. C. (1932). *Remembering: A study in experimental and social psychology*. Cambridge University Press.

Bommasani, R., et al. (2021). On the opportunities and risks of foundation models. *arXiv:2108.07258*.

Brown, T. B., et al. (2020). Language models are few-shot learners. *NeurIPS*, 33, 1877-1901.

Craik, F. I. M., & Tulving, E. (1975). Depth of processing and the retention of words in episodic memory. *Journal of Experimental Psychology: General*, 104(3), 268-294.

Floridi, L. (2011). *The Philosophy of Information*. Oxford University Press.

Johnson, M. K., & Raye, C. L. (1981). Reality monitoring. *Psychological Review*, 88(1), 67-85.

Johnson, M. K., Hashtroudi, S., & Lindsay, D. S. (1993). Source monitoring. *Psychological Bulletin*, 114(1), 3-28.

Liu, N. F., et al. (2023). Lost in the middle: How language models use long contexts. *arXiv:2307.03172*.

Mitchell, K. J., & Johnson, M. K. (2000). Source monitoring: Attributing mental experiences. In E. Tulving & F. I. M. Craik (Eds.), *The Oxford handbook of memory* (pp. 179-195). Oxford University Press.

Schacter, D. L., Norman, K. A., & Koutstaal, W. (1998). The cognitive neuroscience of constructive memory. *Annual Review of Psychology*, 49, 289-318.

Shannon, C. E. (1948). A mathematical theory of communication. *Bell System Technical Journal*, 27(3), 379-423.

---

## Acknowledgments

We thank the developers of GPT-4 (OpenAI), Claude-3.5-Sonnet (Anthropic), and Gemini-2.5-Flash (Google) for API access. This research was conducted independently without external funding.

## Author Contributions

H.D. designed experiments, collected data, performed analyses, and wrote the manuscript.

## Competing Interests

The author declares no competing interests.

## Data Availability

All experimental data and analysis code are available at: https://github.com/HillaryDanan/recursive-reality under MIT license.

---

**Word Count**: TBD